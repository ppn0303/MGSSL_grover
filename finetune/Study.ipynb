{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab3d0ec-9208-4abf-a2a1-336a52dfc06a",
   "metadata": {},
   "source": [
    "# 최초 설정ㅡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92ee7ac3-f96b-408c-97c8-f52385b0926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, shutil\n",
    "import argparse\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from loader import MoleculeDataset, MoleculeDataset_other\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from model import GNN, GNN_graphpred\n",
    "from util import calcul_loss, save_cp, confusion_mat, makedirs, create_logger\n",
    "from splitters import scaffold_split, random_split\n",
    "\n",
    "from rdkit import RDLogger\n",
    "import logging\n",
    "from logging import Logger\n",
    "\n",
    "# i don't want see warning of torch dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7330ba30-f7a5-43fa-8e69-265d24be4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of pre-training of graph neural networks')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                    help='which gpu to use if any (default: 0)')\n",
    "parser.add_argument('--batch_size', type=int, default=32,\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--lr_scale', type=float, default=1,\n",
    "                    help='relative learning rate for the feature extraction layer (default: 1)')\n",
    "parser.add_argument('--decay', type=float, default=1e-7,\n",
    "                    help='weight decay (default: 0)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                    help='number of GNN message passing layers (default: 5).')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                    help='embedding dimensions (default: 300)')\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.2,\n",
    "                    help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling', type=str, default=\"mean\",\n",
    "                    help='graph level pooling (sum, mean, max, set2set, attention)')\n",
    "parser.add_argument('--JK', type=str, default=\"last\",\n",
    "                    help='how the node features across layers are combined. last, sum, max or concat')\n",
    "parser.add_argument('--gnn_type', type=str, default=\"gin\")\n",
    "parser.add_argument('--dataset', type=str, default = 'sider', help='root directory of dataset. For now, only classification.')\n",
    "parser.add_argument('--input_model_file', type=str, default = 'pretrained.pth', help='filename to read the model (if there is any)')\n",
    "parser.add_argument('--output_path', type=str, default = 'output', help='output filename')\n",
    "parser.add_argument('--seed', type=int, default=3, help = \"Seed for splitting the dataset.\")\n",
    "parser.add_argument('--runseed', type=int, default=3, help = \"Seed for minibatch selection, random initialization.\")\n",
    "parser.add_argument('--split', type = str, default=\"scaffold\", help = \"random or scaffold or random_scaffold\")\n",
    "parser.add_argument('--eval_train', type=int, default = 1, help='evaluating training or not')\n",
    "parser.add_argument('--num_workers', type=int, default = 4, help='number of workers for dataset loading')\n",
    "# For search\n",
    "parser.add_argument('--randomsearch', action='store_true', default=False, help='randomsearch mode')\n",
    "parser.add_argument('--gridsearch', action='store_true', default=False, help='gridsearch mode')\n",
    "parser.add_argument('--n_iters', type=int, default=1,\n",
    "                    help='Number of search')\n",
    "args = parser.parse_args(['--dataset','tox21', '--epochs', '2', '--output_path', 'output/tox21'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6222d8dd-bc65-435f-95f1-2ede2807b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'toxcast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "40d593cd-8d4e-4e5c-bcb9-bed247e9ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MoleculeDataset_other(\"dataset/\" + args.dataset, dataset=args.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff622b6-afed-46c2-b2b1-4a9d4c0020da",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad7209f-383b-4959-b844-e685ca6f2123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "\n",
    "def train(args, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    iter_count = 0\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        y = batch.y.view(pred.shape).to(torch.float64)\n",
    "\n",
    "        #loss matrix after removing null target\n",
    "        loss = calcul_loss(pred, y, criterion)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss\n",
    "        iter_count += 1\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    return loss_sum / iter_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743da55-4b71-4da5-9d2a-886a89d58a88",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ffc1b5-c17d-4ad2-9b94-8eadbb14c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    cum_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            y = batch.y.view(pred.shape)\n",
    "            loss = calcul_loss(pred, y, criterion)\n",
    "        \n",
    "        cum_loss += loss\n",
    "        y_true.append(y)\n",
    "        y_scores.append(pred)\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    y_scores = torch.cat(y_scores, dim = 0).cpu().numpy()\n",
    "\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        #AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == -1) > 0:\n",
    "            is_valid = y_true[:,i]**2 > 0\n",
    "            roc_list.append(roc_auc_score((y_true[is_valid,i] + 1)/2, y_scores[is_valid,i]))\n",
    "\n",
    "    if len(roc_list) < y_true.shape[1]:\n",
    "        print(\"Some target is missing!\")\n",
    "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list))/y_true.shape[1]))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    return cum_loss, sum(roc_list)/len(roc_list) #y_true.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae2002-aa14-40c9-9675-a580467bc671",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58f92e4b-dd5f-4b2c-8fec-e8cd46a8f87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(args, model, device, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    cum_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            y = batch.y.view(pred.shape)\n",
    "            loss = calcul_loss(pred, y, criterion)\n",
    "\n",
    "        y_true.append(y)\n",
    "        y_scores.append(pred)\n",
    "        cum_loss += loss\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    y_scores = torch.cat(y_scores, dim = 0).cpu().numpy()\n",
    "\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    rec_list = []\n",
    "    prec_list = []\n",
    "    f1s_list = []\n",
    "    BA_list = []\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    tn_list = []\n",
    "    fn_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = confusion_mat(y_true[:,i], y_scores[:,i])\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "        rec_list.append(rec)\n",
    "        prec_list.append(prec)\n",
    "        f1s_list.append(f1s)\n",
    "        BA_list.append(BA)\n",
    "        tp_list.append(tp)\n",
    "        fp_list.append(fp)\n",
    "        tn_list.append(tn)\n",
    "        fn_list.append(fn)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return cum_loss, auc_list, acc_list, rec_list, prec_list, f1s_list, BA_list, tp_list, fp_list, tn_list, fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "602c86d9-cc07-4a54-b5fb-94181df26683",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger(name='train', save_dir=args.output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "31522c1d-d95d-4908-91d6-9b028f6ea6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:01:52] WARNING: not removing hydrogen atom without neighbors\n",
      "scaffold_balanced_split\n",
      "scaffold_balanced_split\n",
      "total_size:8576 train_size:6860 val_size:858 test_size:858\n",
      "total_size:8576 train_size:6860 val_size:858 test_size:858\n"
     ]
    }
   ],
   "source": [
    "info = logger.info if logger is not None else print\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "#set up dataset\n",
    "dataset = MoleculeDataset_other(\"dataset/\" + args.dataset, dataset=args.dataset)\n",
    "num_tasks = len(dataset[0]['y'])\n",
    "\n",
    "\n",
    "if args.split == \"scaffold\":\n",
    "    smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "    train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "    info(f'scaffold_balanced_split')\n",
    "elif args.split == \"random\":\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "    info(\"random\")\n",
    "elif args.split == \"random_scaffold\":\n",
    "    smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "    train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "    info(\"random scaffold\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid split option.\")\n",
    "\n",
    "info(f'total_size:{len(dataset)} train_size:{len(train_dataset)} val_size:{len(valid_dataset)} test_size:{len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "88fb3a97-a000-484c-8289-b1d376c75dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 12]' is invalid for input of size 19744",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-6a0c39e2d407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcul_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 12]' is invalid for input of size 19744"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_scores = []\n",
    "cum_loss = 0\n",
    "\n",
    "for step, batch in enumerate(test_loader):\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        y = batch.y.view(pred.shape)\n",
    "        loss = calcul_loss(pred, y, criterion)\n",
    "\n",
    "    y_true.append(y)\n",
    "    y_scores.append(pred)\n",
    "    cum_loss += loss\n",
    "\n",
    "y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "y_scores = torch.cat(y_scores, dim = 0).cpu().numpy()\n",
    "for i in range(y_true.shape[1]):\n",
    "    try : auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = confusion_mat(y_true[:,i], y_scores[:,i])\n",
    "    except : print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd46e3b-ddcf-40e6-93b9-29fcbafcf58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d051ed5-a2f1-44c8-8486-c9d28a875e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_true[:,105]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74fb2bb1-e71f-4b7e-bc75-586aa596196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast=pd.read_csv('dataset/toxcast/raw/toxcast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "483f2dc4-bc4e-4bb7-be4d-d4ff85319b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = y_true**2>0\n",
    "targets = (y_true[valid]+1)/2\n",
    "preds = y_scores[valid]\n",
    "hard_preds = [1 if p > 0.5 else 0 for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d883e0db-a92f-4b31-b24c-9e92da0bf39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4942134768920143,\n",
       " 0.8791672250001763,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0,\n",
       " 0,\n",
       " 124658,\n",
       " 17133)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mat(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "884cd230-9a46-4cc1-b543-c3a8c433b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(targets, hard_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a41ebd1a-918b-4b7f-9d40-196bdd8d6b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>ACEA_T47D_80hr_Negative</th>\n",
       "      <th>ACEA_T47D_80hr_Positive</th>\n",
       "      <th>APR_HepG2_CellCycleArrest_24h_dn</th>\n",
       "      <th>APR_HepG2_CellCycleArrest_24h_up</th>\n",
       "      <th>APR_HepG2_CellCycleArrest_72h_dn</th>\n",
       "      <th>APR_HepG2_CellLoss_24h_dn</th>\n",
       "      <th>APR_HepG2_CellLoss_72h_dn</th>\n",
       "      <th>APR_HepG2_MicrotubuleCSK_24h_dn</th>\n",
       "      <th>APR_HepG2_MicrotubuleCSK_24h_up</th>\n",
       "      <th>...</th>\n",
       "      <th>Tanguay_ZF_120hpf_OTIC_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_PE_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_PFIN_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_PIG_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_SNOU_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_SOMI_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_SWIM_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_TRUN_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_TR_up</th>\n",
       "      <th>Tanguay_ZF_120hpf_YSE_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O-][N+](=O)C1=CC=C(Cl)C=C1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[SiH](C)O[Si](C)(C)O[Si](C)(C)O[SiH](C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN1CCN(CC1)C(=O)C1CCCCC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC1=CC=C(C=C1)[N+]([O-])=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OC1=CC=C(C=C1)[N+]([O-])=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>[O-]S(=O)(=O)C(F)(F)F.CCCC[N+]1=CC=CC=C1C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>F[P-](F)(F)(F)(F)F.CCCC[N+]1=CC=CC=C1C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>[O-]S(=O)(=O)C(F)(F)F.CCC[N+]1(C)CCCC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>CCCCCCCCCCCCC1=CC=CC=C1S([O-])(=O)=O.CCCCCCCCC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>CN1CC2=C(N[C@H](CC(O)=O)C1=O)C=CC(=C2)C(=O)N1C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8576 rows × 618 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  \\\n",
       "0                           [O-][N+](=O)C1=CC=C(Cl)C=C1   \n",
       "1             C[SiH](C)O[Si](C)(C)O[Si](C)(C)O[SiH](C)C   \n",
       "2                              CN1CCN(CC1)C(=O)C1CCCCC1   \n",
       "3                            NC1=CC=C(C=C1)[N+]([O-])=O   \n",
       "4                            OC1=CC=C(C=C1)[N+]([O-])=O   \n",
       "...                                                 ...   \n",
       "8571          [O-]S(=O)(=O)C(F)(F)F.CCCC[N+]1=CC=CC=C1C   \n",
       "8572             F[P-](F)(F)(F)(F)F.CCCC[N+]1=CC=CC=C1C   \n",
       "8573             [O-]S(=O)(=O)C(F)(F)F.CCC[N+]1(C)CCCC1   \n",
       "8574  CCCCCCCCCCCCC1=CC=CC=C1S([O-])(=O)=O.CCCCCCCCC...   \n",
       "8575  CN1CC2=C(N[C@H](CC(O)=O)C1=O)C=CC(=C2)C(=O)N1C...   \n",
       "\n",
       "      ACEA_T47D_80hr_Negative  ACEA_T47D_80hr_Positive  \\\n",
       "0                         0.0                      0.0   \n",
       "1                         NaN                      NaN   \n",
       "2                         NaN                      NaN   \n",
       "3                         1.0                      0.0   \n",
       "4                         0.0                      0.0   \n",
       "...                       ...                      ...   \n",
       "8571                      NaN                      NaN   \n",
       "8572                      NaN                      NaN   \n",
       "8573                      NaN                      NaN   \n",
       "8574                      NaN                      NaN   \n",
       "8575                      NaN                      NaN   \n",
       "\n",
       "      APR_HepG2_CellCycleArrest_24h_dn  APR_HepG2_CellCycleArrest_24h_up  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                                  NaN                               NaN   \n",
       "2                                  NaN                               NaN   \n",
       "3                                  0.0                               0.0   \n",
       "4                                  0.0                               0.0   \n",
       "...                                ...                               ...   \n",
       "8571                               NaN                               NaN   \n",
       "8572                               NaN                               NaN   \n",
       "8573                               NaN                               NaN   \n",
       "8574                               NaN                               NaN   \n",
       "8575                               NaN                               NaN   \n",
       "\n",
       "      APR_HepG2_CellCycleArrest_72h_dn  APR_HepG2_CellLoss_24h_dn  \\\n",
       "0                                  NaN                        NaN   \n",
       "1                                  NaN                        NaN   \n",
       "2                                  NaN                        NaN   \n",
       "3                                  0.0                        0.0   \n",
       "4                                  0.0                        0.0   \n",
       "...                                ...                        ...   \n",
       "8571                               NaN                        NaN   \n",
       "8572                               NaN                        NaN   \n",
       "8573                               NaN                        NaN   \n",
       "8574                               NaN                        NaN   \n",
       "8575                               NaN                        NaN   \n",
       "\n",
       "      APR_HepG2_CellLoss_72h_dn  APR_HepG2_MicrotubuleCSK_24h_dn  \\\n",
       "0                           NaN                              NaN   \n",
       "1                           NaN                              NaN   \n",
       "2                           NaN                              NaN   \n",
       "3                           0.0                              0.0   \n",
       "4                           0.0                              0.0   \n",
       "...                         ...                              ...   \n",
       "8571                        NaN                              NaN   \n",
       "8572                        NaN                              NaN   \n",
       "8573                        NaN                              NaN   \n",
       "8574                        NaN                              NaN   \n",
       "8575                        NaN                              NaN   \n",
       "\n",
       "      APR_HepG2_MicrotubuleCSK_24h_up  ...  Tanguay_ZF_120hpf_OTIC_up  \\\n",
       "0                                 NaN  ...                        0.0   \n",
       "1                                 NaN  ...                        NaN   \n",
       "2                                 NaN  ...                        NaN   \n",
       "3                                 0.0  ...                        0.0   \n",
       "4                                 0.0  ...                        0.0   \n",
       "...                               ...  ...                        ...   \n",
       "8571                              NaN  ...                        NaN   \n",
       "8572                              NaN  ...                        NaN   \n",
       "8573                              NaN  ...                        NaN   \n",
       "8574                              NaN  ...                        NaN   \n",
       "8575                              NaN  ...                        NaN   \n",
       "\n",
       "      Tanguay_ZF_120hpf_PE_up  Tanguay_ZF_120hpf_PFIN_up  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         NaN                        NaN   \n",
       "2                         NaN                        NaN   \n",
       "3                         0.0                        0.0   \n",
       "4                         0.0                        0.0   \n",
       "...                       ...                        ...   \n",
       "8571                      NaN                        NaN   \n",
       "8572                      NaN                        NaN   \n",
       "8573                      NaN                        NaN   \n",
       "8574                      NaN                        NaN   \n",
       "8575                      NaN                        NaN   \n",
       "\n",
       "      Tanguay_ZF_120hpf_PIG_up  Tanguay_ZF_120hpf_SNOU_up  \\\n",
       "0                          0.0                        0.0   \n",
       "1                          NaN                        NaN   \n",
       "2                          NaN                        NaN   \n",
       "3                          0.0                        0.0   \n",
       "4                          0.0                        0.0   \n",
       "...                        ...                        ...   \n",
       "8571                       NaN                        NaN   \n",
       "8572                       NaN                        NaN   \n",
       "8573                       NaN                        NaN   \n",
       "8574                       NaN                        NaN   \n",
       "8575                       NaN                        NaN   \n",
       "\n",
       "      Tanguay_ZF_120hpf_SOMI_up  Tanguay_ZF_120hpf_SWIM_up  \\\n",
       "0                           0.0                        0.0   \n",
       "1                           NaN                        NaN   \n",
       "2                           NaN                        NaN   \n",
       "3                           0.0                        0.0   \n",
       "4                           0.0                        0.0   \n",
       "...                         ...                        ...   \n",
       "8571                        NaN                        NaN   \n",
       "8572                        NaN                        NaN   \n",
       "8573                        NaN                        NaN   \n",
       "8574                        NaN                        NaN   \n",
       "8575                        NaN                        NaN   \n",
       "\n",
       "      Tanguay_ZF_120hpf_TRUN_up  Tanguay_ZF_120hpf_TR_up  \\\n",
       "0                           0.0                      0.0   \n",
       "1                           NaN                      NaN   \n",
       "2                           NaN                      NaN   \n",
       "3                           0.0                      0.0   \n",
       "4                           0.0                      0.0   \n",
       "...                         ...                      ...   \n",
       "8571                        NaN                      NaN   \n",
       "8572                        NaN                      NaN   \n",
       "8573                        NaN                      NaN   \n",
       "8574                        NaN                      NaN   \n",
       "8575                        NaN                      NaN   \n",
       "\n",
       "      Tanguay_ZF_120hpf_YSE_up  \n",
       "0                          0.0  \n",
       "1                          NaN  \n",
       "2                          NaN  \n",
       "3                          0.0  \n",
       "4                          0.0  \n",
       "...                        ...  \n",
       "8571                       NaN  \n",
       "8572                       NaN  \n",
       "8573                       NaN  \n",
       "8574                       NaN  \n",
       "8575                       NaN  \n",
       "\n",
       "[8576 rows x 618 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "242883cd-ae24-49fc-af3c-de315ad28cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686 is all 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(858):\n",
    "    if sum(y_true[i])==858:\n",
    "        print(f'{i} is all 1')\n",
    "    elif sum(y_true[i])==0:\n",
    "        print(f'{i} is all 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897dbfcc-088e-4a0b-803a-903a34c77f5d",
   "metadata": {},
   "source": [
    "## runtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d212be3a-176e-4e3e-8179-314a65d2f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args: Namespace, logger: Logger = None):\n",
    "    info = logger.info if logger is not None else print\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "    #set up dataset\n",
    "    dataset = MoleculeDataset_other(\"dataset/\" + args.dataset, dataset=args.dataset)\n",
    "    num_tasks = len(dataset[0]['y'])\n",
    "\n",
    "    \n",
    "    if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "        info(f'scaffold_balanced_split')\n",
    "    elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        info(\"random\")\n",
    "    elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        info(\"random scaffold\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "        \n",
    "    info(f'total_size:{len(dataset)} train_size:{len(train_dataset)} val_size:{len(valid_dataset)} test_size:{len(test_dataset)}')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n",
    "    val_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "\n",
    "    #set up model\n",
    "    model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "    if not args.input_model_file == \"\":\n",
    "        model.from_pretrained(args.input_model_file)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    #set up optimizer\n",
    "    #different learning rate for different part of GNN\n",
    "    model_param_group = []\n",
    "    model_param_group.append({\"params\": model.gnn.parameters()})\n",
    "    if args.graph_pooling == \"attention\":\n",
    "        model_param_group.append({\"params\": model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "    model_param_group.append({\"params\": model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "    optimizer = optim.Adam(model_param_group, lr=args.lr, weight_decay=args.decay)\n",
    "    info(optimizer)\n",
    "\n",
    "    best_val_loss = 9999\n",
    "    best_model_path = os.path.join(args.output_path, str(args.seed))\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        info(\"====epoch \" + str(epoch))\n",
    "        tst = time.time()\n",
    "        train_loss = train(args, model, device, train_loader, optimizer)\n",
    "        tet = time.time() - tst\n",
    "        info(\"====Evaluation\")\n",
    "        vst = time.time()\n",
    "        val_loss, val_auc = valid(args, model, device, val_loader)\n",
    "        vet = time.time() - vst\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_cp(args, model, path=best_model_path)\n",
    "        info(f'train_loss:{train_loss:.4f} val_loss:{val_loss:.4f} val_auc:{val_auc:.4f} t_time:{tet} v_time:{vet}')\n",
    "    \n",
    "    best_state = torch.load(os.path.join(best_model_path,'model.pt'))\n",
    "    model.load_state_dict(best_state['state_dict'])\n",
    "    \n",
    "    test_loss, auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = test(args, model, device, test_loader)\n",
    "    avg_auc = sum(auc)/num_tasks\n",
    "    avg_acc = sum(acc)/num_tasks\n",
    "    avg_rec = sum(rec)/num_tasks\n",
    "    avg_prec = sum(prec)/num_tasks\n",
    "    avg_f1s = sum(f1s)/num_tasks\n",
    "    avg_BA = sum(BA)/num_tasks\n",
    "    avg_tp = sum(tp)/num_tasks\n",
    "    avg_fp = sum(fp)/num_tasks\n",
    "    avg_tn = sum(tn)/num_tasks\n",
    "    avg_fn = sum(fn)/num_tasks\n",
    "        \n",
    "    info(f'seed:{args.seed} loss:{test_loss} auc:{avg_auc} acc:{avg_acc} rec:{avg_rec} prec:{avg_prec} f1:{avg_f1s} BA:{avg_BA}\\ntp:{avg_tp} fp:{avg_fp} fn:{avg_fn} tn:{avg_tn}')\n",
    "    #delete for memory\n",
    "    del train_dataset, valid_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "\n",
    "    return avg_auc, avg_acc, avg_rec, avg_prec, avg_f1s, avg_BA, avg_tp, avg_fp, avg_tn, avg_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb86c00-34fe-4559-bc08-8d57e71322c5",
   "metadata": {},
   "source": [
    "# cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d8b879-d6b4-4e2a-aa49-c0341e1fd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(args: Namespace, logger: Logger = None):\n",
    "    info = logger.info if logger is not None else print\n",
    "    \n",
    "    if not os.path.exists(args.output_path):\n",
    "        os.makedirs(args.output_path)\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    rec_list = []\n",
    "    prec_list = []\n",
    "    f1s_list = []\n",
    "    BA_list = []\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    tn_list = []\n",
    "    fn_list = []\n",
    "    for k in range(3):\n",
    "        auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = run_training(args)\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "        rec_list.append(rec)\n",
    "        prec_list.append(prec)\n",
    "        f1s_list.append(f1s)\n",
    "        BA_list.append(BA)\n",
    "        tp_list.append(tp)\n",
    "        fp_list.append(fp)\n",
    "        tn_list.append(tn)\n",
    "        fn_list.append(fn)\n",
    "        args.seed += 1\n",
    "    info(f'all test end')\n",
    "    info(f'overall test_auc : {np.nanmean(auc_list):.4f}\\nstd={np.nanstd(auc_list):.4f}')\n",
    "    info(f'overall test_accuracy : {np.nanmean(acc_list):.4f}\\nstd={np.nanstd(acc_list):.4f}')\n",
    "    info(f'overall test_recall : {np.nanmean(rec_list):.4f}\\nstd={np.nanstd(rec_list):.4f}')\n",
    "    info(f'overall test_precision : {np.nanmean(prec_list):.4f}\\nstd={np.nanstd(prec_list):.4f}')\n",
    "    info(f'overall test_f1score : {np.nanmean(f1s_list):.4f}\\nstd={np.nanstd(f1s_list):.4f}')\n",
    "    info(f'overall test_Balanced_Accuracy : {np.nanmean(BA_list):.4f}\\nstd={np.nanstd(BA_list):.4f}')\n",
    "    info(f'overall test_tp : {np.nanmean(tp_list):.2f}\\nstd={np.nanstd(tp_list):.2f}')\n",
    "    info(f'overall test_fp : {np.nanmean(fp_list):.2f}\\nstd={np.nanstd(fp_list):.2f}')\n",
    "    info(f'overall test_fn : {np.nanmean(fn_list):.2f}\\nstd={np.nanstd(fn_list):.2f}')\n",
    "    info(f'overall test_tn : {np.nanmean(tn_list):.2f}\\nstd={np.nanstd(tn_list):.2f}')\n",
    "    info(f'\\n       (pred)pos    neg(pred)')\n",
    "    info(f'pos(true)    {tp:.2f}  {fn:.2f}')\n",
    "    info(f'neg(true)    {fp:.2f}  {tn:.2f}')\n",
    "    \n",
    "    return np.nanmean(auc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5c5d0-734b-4ef8-be09-52fe664ed216",
   "metadata": {},
   "source": [
    "# random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "097d5b74-f564-47e9-b637-7bdf13f10161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(args: Namespace, logger: Logger = None):\n",
    "    info = logger.info if logger is not None else print\n",
    "    \n",
    "    init_seed = args.seed\n",
    "    save_dir = args.output_path\n",
    "\n",
    "    #randomize parameter list\n",
    "    lr_list = [0.0005, 0.00075, 0.001, 0.00125, 0.0015, 0.00175, 0.002]\n",
    "    dropout_list = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "    gpooling_list = ['mean', 'sum']\n",
    "    lr_scale_list = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\n",
    "\n",
    "    # Run training with different random seeds for each fold\n",
    "    all_scores = []\n",
    "    params = []\n",
    "    for iter_num in range(0, args.n_iters):\n",
    "        info(f'iter {iter_num}')\n",
    "\n",
    "        #randomize parameter\n",
    "        np.random.seed()\n",
    "        random.seed()\n",
    "        args.lr = np.random.choice(lr_list, 1)[0]\n",
    "        args.dropout_ratio = np.random.choice(dropout_list, 1)[0]\n",
    "        params.append(f'\\n{iter_num}th search parameter : lr is {args.lr} \\n dropout is {args.dropout_ratio} \\n batch_size is {args.batch_size}')\n",
    "        info(params[iter_num])\n",
    "\n",
    "        args.seed = init_seed                        # if change this, result will be change\n",
    "        iter_dir = os.path.join(save_dir, f'iter_{iter_num}')\n",
    "        args.output_path = iter_dir\n",
    "        makedirs(args.output_path)\n",
    "\n",
    "        iter_score = cross_validate(args, logger)\n",
    "        all_scores.append(iter_score)\n",
    "\n",
    "        if max(all_scores)==iter_score : \n",
    "            best_iter = iter_num\n",
    "            best_score = iter_score\n",
    "            best_param = params[iter_num]\n",
    "############iter end\n",
    "\n",
    "    all_scores = np.array(all_scores)\n",
    "\n",
    "    # Report scores for each iter\n",
    "    info(f'\\n---- {args.n_iters}-iter random search ----')\n",
    "\n",
    "    for iter_num, scores in enumerate(all_scores):\n",
    "        info(params[iter_num])\n",
    "        info(f'Seed {init_seed} ==> test AUC = {np.nanmean(scores):.6f}\\n')\n",
    "\n",
    "    # Report best model\n",
    "    info(f'\\nbest_iter : {best_iter}\\nbest_score is {np.nanmean(best_score)}\\nbest_param : {best_param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4476f1c6-5b81-46ef-8b6c-d76ca87153b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpooling_list = ['mean', 'sum']\n",
    "JK_list = ['last', 'sum', 'concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c13b952-2714-42b7-b067-85eb8575b38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(JK_list, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928136e-c10a-4aaf-a578-87531d7a25c1",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7325ae38-fb63-492c-a25f-7ac99a999d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "import logging\n",
    "from logging import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3950f1d-cbf4-4a23-80d8-a4f0f080119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import calcul_loss, save_cp, confusion_mat, makedirs, create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79af60a6-3b1e-4472-ac68-26e5b66c2cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold_balanced_split\n",
      "total_size:7831 train_size:6264 val_size:783 test_size:784\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "====epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-06999a278f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7815f9139334>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mauc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e497ab643f22>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mtet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====Evaluation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5dad83c1544b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "logger = create_logger(name='train', save_dir=args.output_path, quiet=False)\n",
    "if args.randomsearch:\n",
    "    best_metric = 0\n",
    "    random_search(args, logger)\n",
    "else : \n",
    "    cross_validate(args, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db7fe2-1b28-4f1b-8555-e56880e11644",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d967fc-e7c9-4a4c-a5e3-4f2a1c9870e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "from torch.utils import data\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from itertools import repeat, product, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba0cc43-c45e-41ec-8d6b-32f845c9666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_sider_dataset(input_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_path:\n",
    "    :return: list of smiles, list of rdkit mol obj, np.array containing the\n",
    "    labels\n",
    "    \"\"\"\n",
    "    input_df = pd.read_csv(input_path, sep=',')\n",
    "    smiles_list = input_df['smiles']\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    tasks = ['Hepatobiliary disorders',\n",
    "       'Metabolism and nutrition disorders', 'Product issues', 'Eye disorders',\n",
    "       'Investigations', 'Musculoskeletal and connective tissue disorders',\n",
    "       'Gastrointestinal disorders', 'Social circumstances',\n",
    "       'Immune system disorders', 'Reproductive system and breast disorders',\n",
    "       'Neoplasms benign, malignant and unspecified (incl cysts and polyps)',\n",
    "       'General disorders and administration site conditions',\n",
    "       'Endocrine disorders', 'Surgical and medical procedures',\n",
    "       'Vascular disorders', 'Blood and lymphatic system disorders',\n",
    "       'Skin and subcutaneous tissue disorders',\n",
    "       'Congenital, familial and genetic disorders',\n",
    "       'Infections and infestations',\n",
    "       'Respiratory, thoracic and mediastinal disorders',\n",
    "       'Psychiatric disorders', 'Renal and urinary disorders',\n",
    "       'Pregnancy, puerperium and perinatal conditions',\n",
    "       'Ear and labyrinth disorders', 'Cardiac disorders',\n",
    "       'Nervous system disorders',\n",
    "       'Injury, poisoning and procedural complications']\n",
    "    labels = input_df[tasks]\n",
    "    # convert 0 to -1\n",
    "    labels = labels.replace(0, -1)\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85859d7-7ffb-43fd-b816-9ada0a1a3c1a",
   "metadata": {},
   "source": [
    "## 2D Mol에 대한 Dataset 생성코드\n",
    "- atomfeature는 2개 밖에 없다. atom type, chirality tag\n",
    "- bondfeature도 2개 밖에 없다. bond type, bond direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd11a9a-b2ca-4ea2-9ef1-e853753b921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowable_features = {\n",
    "    'possible_atomic_num_list' : list(range(1, 119)),\n",
    "    'possible_formal_charge_list' : [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],\n",
    "    'possible_chirality_list' : [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "        Chem.rdchem.ChiralType.CHI_OTHER\n",
    "    ],\n",
    "    'possible_hybridization_list' : [\n",
    "        Chem.rdchem.HybridizationType.S,\n",
    "        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2, Chem.rdchem.HybridizationType.UNSPECIFIED\n",
    "    ],\n",
    "    'possible_numH_list' : [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'possible_implicit_valence_list' : [0, 1, 2, 3, 4, 5, 6],\n",
    "    'possible_degree_list' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'possible_bonds' : [\n",
    "        Chem.rdchem.BondType.SINGLE,\n",
    "        Chem.rdchem.BondType.DOUBLE,\n",
    "        Chem.rdchem.BondType.TRIPLE,\n",
    "        Chem.rdchem.BondType.AROMATIC\n",
    "    ],\n",
    "    'possible_bond_dirs' : [ # only for double bond stereo information\n",
    "        Chem.rdchem.BondDir.NONE,\n",
    "        Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "        Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd439925-420d-4707-8e1b-740dda264e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_graph_data_obj_simple(mol):\n",
    "    \"\"\"\n",
    "    Converts rdkit mol object to graph Data object required by the pytorch\n",
    "    geometric package. NB: Uses simplified atom and bond features, and represent\n",
    "    as indices\n",
    "    :param mol: rdkit mol object\n",
    "    :return: graph data object with the attributes: x, edge_index, edge_attr\n",
    "    \"\"\"\n",
    "    # atoms\n",
    "    num_atom_features = 2   # atom type,  chirality tag\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_feature = [allowable_features['possible_atomic_num_list'].index(\n",
    "            atom.GetAtomicNum())] + [allowable_features[\n",
    "            'possible_chirality_list'].index(atom.GetChiralTag())]\n",
    "        atom_features_list.append(atom_feature)\n",
    "    x = torch.tensor(np.array(atom_features_list), dtype=torch.long)\n",
    "\n",
    "    # bonds\n",
    "    num_bond_features = 2   # bond type, bond direction\n",
    "    if len(mol.GetBonds()) > 0: # mol has bonds\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_feature = [allowable_features['possible_bonds'].index(\n",
    "                bond.GetBondType())] + [allowable_features[\n",
    "                                            'possible_bond_dirs'].index(\n",
    "                bond.GetBondDir())]\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "\n",
    "        # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "        edge_index = torch.tensor(np.array(edges_list).T, dtype=torch.long)\n",
    "\n",
    "        # data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "        edge_attr = torch.tensor(np.array(edge_features_list),\n",
    "                                 dtype=torch.long)\n",
    "    else:   # mol has no bonds\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, num_bond_features), dtype=torch.long)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e8b25-0f94-45a3-ba2f-8310a85ee7a1",
   "metadata": {},
   "source": [
    "### mol_to_graph 세부 시행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ed68eb-5b87-47bf-a99d-735a051491c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [6, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [6, 0],\n",
       "        [6, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [6, 0],\n",
       "        [7, 0],\n",
       "        [7, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = AllChem.MolFromSmiles('N(CC1=CC=CC=C1)(CCC#N)C2=CC=C(N=NC3=CC=C(N(=O)=O)C=C3)C=C2')\n",
    "num_atom_features = 2   # atom type,  chirality tag\n",
    "atom_features_list = []\n",
    "for atom in mol.GetAtoms():\n",
    "    atom_feature = [allowable_features['possible_atomic_num_list'].index(\n",
    "        atom.GetAtomicNum())] + [allowable_features[\n",
    "        'possible_chirality_list'].index(atom.GetChiralTag())]\n",
    "    atom_features_list.append(atom_feature)\n",
    "x = torch.tensor(np.array(atom_features_list), dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a195e1f3-8332-4c09-bb33-7569afdb33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonds\n",
    "num_bond_features = 2   # bond type, bond direction\n",
    "bonds = []\n",
    "if len(mol.GetBonds()) > 0: # mol has bonds\n",
    "    edges_list = []\n",
    "    edge_features_list = []\n",
    "    for bond in mol.GetBonds():\n",
    "        bonds.append(bond)\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_feature = [allowable_features['possible_bonds'].index(\n",
    "            bond.GetBondType())] + [allowable_features[\n",
    "                                        'possible_bond_dirs'].index(\n",
    "            bond.GetBondDir())]\n",
    "        edges_list.append((i, j))\n",
    "        edge_features_list.append(edge_feature)\n",
    "        edges_list.append((j, i))\n",
    "        edge_features_list.append(edge_feature)\n",
    "\n",
    "    # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "    edge_index = torch.tensor(np.array(edges_list).T, dtype=torch.long)\n",
    "\n",
    "    # data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "    edge_attr = torch.tensor(np.array(edge_features_list),\n",
    "                             dtype=torch.long)\n",
    "else:   # mol has no bonds\n",
    "    edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    edge_attr = torch.empty((0, num_bond_features), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485d650b-3187-4e85-a85e-983d83aa2209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowable_features['possible_bonds'].index(bonds[0].GetBondType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a384f57-9ff4-4960-acb0-85b41d91f958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[allowable_features['possible_bond_dirs'].index(bonds[0].GetBondDir())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb83a3a1-fbc7-4c30-b556-0fdd0a73490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "The class to store Bonds.\n",
       "Note: unlike Atoms, is it currently impossible to construct Bonds from\n",
       "Python.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Raises an exception\n",
       "This class cannot be instantiated from Python\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/rdkit/Chem/rdchem.so\n",
       "\u001b[0;31mType:\u001b[0m           class\n",
       "\u001b[0;31mSubclasses:\u001b[0m     QueryBond\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Chem.rdchem.Bond?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf20c7c-0d20-4c46-aea4-b302305a7ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[29, 2], edge_index=[2, 62], edge_attr=[62, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = AllChem.MolFromSmiles('N(CC1=CC=CC=C1)(CCC#N)C2=CC=C(N=NC3=CC=C(N(=O)=O)C=C3)C=C2')\n",
    "graph = mol_to_graph_data_obj_simple(mol)\n",
    "graph\n",
    "# dataset 대비 id, y만 차이난다.\n",
    "# id는 말그대로 dataset에서 몇번째인지, y는 label을 의미한다.\n",
    "# edge_attr : Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "# edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "# x : atomfeature으로 atom번호, chirality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec9a842-3e06-408d-9c4b-70aa37e58150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [2, 0],\n",
       "        [2, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0],\n",
       "        [3, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9525abec-230c-4bec-8f64-38eb7f6036bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  0,  8,  8,  9,\n",
       "          9, 10, 10, 11,  0, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18,\n",
       "         18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 22, 24, 21, 25, 25, 26, 15, 27,\n",
       "         27, 28,  7,  2, 28, 12, 26, 18],\n",
       "        [ 1,  0,  2,  1,  3,  2,  4,  3,  5,  4,  6,  5,  7,  6,  8,  0,  9,  8,\n",
       "         10,  9, 11, 10, 12,  0, 13, 12, 14, 13, 15, 14, 16, 15, 17, 16, 18, 17,\n",
       "         19, 18, 20, 19, 21, 20, 22, 21, 23, 22, 24, 22, 25, 21, 26, 25, 27, 15,\n",
       "         28, 27,  2,  7, 12, 28, 18, 26]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fc3416-9f1a-405e-aa09-c914348b7dc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cf73d7fed589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_embedding1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.gnn.x_embedding1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fa3674-f610-4d0b-87ab-9cc5e837ee1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b5f78147cc77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_embedding1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0memb1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "emb1 = model.gnn.x_embedding1(graph.edge_attr[:,0])\n",
    "emb1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c0d11603-eab3-47d2-9bf9-f4355182fad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 300])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "52ccbf34-a342-47db-baa1-7ac0405af332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  0,  8,  8,  9,\n",
       "          9, 10, 10, 11,  0, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18,\n",
       "         18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 22, 24, 21, 25, 25, 26, 15, 27,\n",
       "         27, 28,  7,  2, 28, 12, 26, 18],\n",
       "        [ 1,  0,  2,  1,  3,  2,  4,  3,  5,  4,  6,  5,  7,  6,  8,  0,  9,  8,\n",
       "         10,  9, 11, 10, 12,  0, 13, 12, 14, 13, 15, 14, 16, 15, 17, 16, 18, 17,\n",
       "         19, 18, 20, 19, 21, 20, 22, 21, 23, 22, 24, 22, 25, 21, 26, 25, 27, 15,\n",
       "         28, 27,  2,  7, 12, 28, 18, 26]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cd4d62e4-a3ed-4d0c-ab2c-444195baff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [6, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [6, 0],\n",
       "        [6, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [6, 0],\n",
       "        [7, 0],\n",
       "        [7, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0],\n",
       "        [5, 0]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddafa0-3188-49c4-9599-5350340d2d0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## change load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9610b4dc-6f4d-4194-b425-961efbd3f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_other_dataset(input_path):\n",
    "    \"\"\"\n",
    "    this is for loading other datasets\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    smiles_list = df.iloc[:,0]\n",
    "    rdkit_mol_objs_list = [AllChem.MolFromSmiles(s) for s in smiles_list]\n",
    "    tasks = list(df.columns[1:])\n",
    "    labels = df[tasks]\n",
    "    labels = labels.replace(0, -1)\n",
    "    assert len(smiles_list) == len(rdkit_mol_objs_list)\n",
    "    assert len(smiles_list) == len(labels)\n",
    "    return smiles_list, rdkit_mol_objs_list, labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c5f786d-a1a0-4d34-8752-9d3c544e372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Tuple, Union, Dict\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "from logging import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebee21da-4f75-4aba-b815-02974c134fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset_other(InMemoryDataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 #data = None,\n",
    "                 #slices = None,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None,\n",
    "                 dataset='zinc250k',\n",
    "                 empty=False):\n",
    "        \"\"\"\n",
    "        Adapted from qm9.py. Disabled the download functionality\n",
    "        :param root: directory of the dataset, containing a raw and processed\n",
    "        dir. The raw dir should contain the file containing the smiles, and the\n",
    "        processed dir can either empty or a previously processed file\n",
    "        :param dataset: name of the dataset. Currently only implemented for\n",
    "        zinc250k, chembl_with_labels, tox21, hiv, bace, bbbp, clintox, esol,\n",
    "        freesolv, lipophilicity, muv, pcba, sider, toxcast\n",
    "        :param empty: if True, then will not load any data obj. For\n",
    "        initializing empty dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.root = root\n",
    "        self.smiles = []\n",
    "\n",
    "        super(MoleculeDataset_other, self).__init__(root, transform, pre_transform,\n",
    "                                                 pre_filter)\n",
    "        self.transform, self.pre_transform, self.pre_filter = transform, pre_transform, pre_filter\n",
    "        if not empty:\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = Data()\n",
    "        for key in self.data.keys:\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            s = list(repeat(slice(None), item.dim()))\n",
    "            s[data.__cat_dim__(key, item)] = slice(slices[idx],\n",
    "                                                    slices[idx + 1])\n",
    "            data[key] = item[s]\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        file_name_list = os.listdir(self.raw_dir)\n",
    "        # assert len(file_name_list) == 1     # currently assume we have a\n",
    "        # # single raw file\n",
    "        return file_name_list\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'geometric_data_processed.pt'\n",
    "\n",
    "    def download(self):\n",
    "        raise NotImplementedError('Must indicate valid location of raw data. '\n",
    "                                  'No download allowed')\n",
    "        \n",
    "    def process(self):\n",
    "        data_smiles_list = []\n",
    "        data_list = []\n",
    "        \n",
    "        smiles_list, rdkit_mol_objs, labels = \\\n",
    "            _load_other_dataset(self.raw_paths[0])\n",
    "        for i in range(len(smiles_list)):\n",
    "            #print(i)\n",
    "            rdkit_mol = rdkit_mol_objs[i]\n",
    "            if rdkit_mol != None:\n",
    "                data = mol_to_graph_data_obj_simple(rdkit_mol)\n",
    "                # manually add mol id\n",
    "                data.id = torch.tensor(\n",
    "                    [i])  # id here is the index of the mol in\n",
    "                # the dataset\n",
    "                data.y = torch.tensor(labels[i, :])\n",
    "                data_list.append(data)\n",
    "                data_smiles_list.append(smiles_list[i])\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        # write data_smiles_list in processed paths\n",
    "        data_smiles_series = pd.Series(data_smiles_list)\n",
    "        data_smiles_series.to_csv(os.path.join(self.processed_dir,\n",
    "                                               'smiles.csv'), index=False,\n",
    "                                  header=False)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "        self.smiles = smiles_list\n",
    "        self.data_list = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d524914e-82fd-4b04-b451-57f3c1d0d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MoleculeDataset_other(root='dataset/tox21', dataset='tox21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b1cbe96-f7ac-413f-ac69-5e52aefcdbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:24:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "smiles_list2 = pd.read_csv('dataset/' + 'tox21' + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "num_tasks = len(data[0]['y'])\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(data, smiles_list2, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "11bf2704-d6f8-4fc1-9156-da1dfc58f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splitters import scaffold_split, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7125c86-5b2f-4d41-b5b8-8db792607bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MoleculeDataset('dataset/tox21', dataset='tox21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e0663d7-2634-401a-9ef6-2ba682da588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[04:23:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "smiles_list = pd.read_csv('dataset/' + 'tox21' + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0362973-6bf7-4efe-a065-60d32e1baa90",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c23f293d-3fc2-43b7-8e1b-bd7b0f15f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d92d0239-30b8-47f8-8666-1d22bc5cd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"\n",
    "    Obtain Bemis-Murcko scaffold from smiles\n",
    "    :param smiles:\n",
    "    :param include_chirality:\n",
    "    :return: smiles of scaffold\n",
    "    \"\"\"\n",
    "    scaffold = MurckoScaffold.MurckoScaffoldSmiles(\n",
    "        smiles=smiles, includeChirality=include_chirality)\n",
    "    return scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "953c0f64-55c9-4c53-aadb-cb8cf57953bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_split(dataset, smiles_list, task_idx=None, null_value=0,\n",
    "                   frac_train=0.8, frac_valid=0.1, frac_test=0.1,\n",
    "                   return_smiles=False):\n",
    "    \"\"\"\n",
    "    Adapted from  https://github.com/deepchem/deepchem/blob/master/deepchem/splits/splitters.py\n",
    "    Split dataset by Bemis-Murcko scaffolds\n",
    "    This function can also ignore examples containing null values for a\n",
    "    selected task when splitting. Deterministic split\n",
    "    :param dataset: pytorch geometric dataset obj\n",
    "    :param smiles_list: list of smiles corresponding to the dataset obj\n",
    "    :param task_idx: column idx of the data.y tensor. Will filter out\n",
    "    examples with null value in specified task column of the data.y tensor\n",
    "    prior to splitting. If None, then no filtering\n",
    "    :param null_value: float that specifies null value in data.y to filter if\n",
    "    task_idx is provided\n",
    "    :param frac_train:\n",
    "    :param frac_valid:\n",
    "    :param frac_test:\n",
    "    :param return_smiles:\n",
    "    :return: train, valid, test slices of the input dataset obj. If\n",
    "    return_smiles = True, also returns ([train_smiles_list],\n",
    "    [valid_smiles_list], [test_smiles_list])\n",
    "    \"\"\"\n",
    "    np.testing.assert_almost_equal(frac_train + frac_valid + frac_test, 1.0)\n",
    "\n",
    "    if task_idx != None:\n",
    "        # filter based on null values in task_idx\n",
    "        # get task array\n",
    "        y_task = np.array([data.y[task_idx].item() for data in dataset])\n",
    "        # boolean array that correspond to non null values\n",
    "        non_null = y_task != null_value\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "    else:\n",
    "        non_null = np.ones(len(dataset)) == 1\n",
    "        smiles_list = list(compress(enumerate(smiles_list), non_null))\n",
    "\n",
    "    # create dict of the form {scaffold_i: [idx1, idx....]}\n",
    "    all_scaffolds = {}\n",
    "    for i, smiles in smiles_list:\n",
    "        scaffold = generate_scaffold(smiles, include_chirality=True)\n",
    "        if scaffold not in all_scaffolds:\n",
    "            all_scaffolds[scaffold] = [i]\n",
    "        else:\n",
    "            all_scaffolds[scaffold].append(i)\n",
    "\n",
    "    # sort from largest to smallest sets\n",
    "    all_scaffolds = {key: sorted(value) for key, value in all_scaffolds.items()}\n",
    "    all_scaffold_sets = [\n",
    "        scaffold_set for (scaffold, scaffold_set) in sorted(\n",
    "            all_scaffolds.items(), key=lambda x: (len(x[1]), x[1][0]), reverse=True)\n",
    "    ]\n",
    "\n",
    "    # get train, valid test indices\n",
    "    train_cutoff = frac_train * len(smiles_list)\n",
    "    valid_cutoff = (frac_train + frac_valid) * len(smiles_list)\n",
    "    train_idx, valid_idx, test_idx = [], [], []\n",
    "    for scaffold_set in all_scaffold_sets:\n",
    "        if len(train_idx) + len(scaffold_set) > train_cutoff:\n",
    "            if len(train_idx) + len(valid_idx) + len(scaffold_set) > valid_cutoff:\n",
    "                test_idx.extend(scaffold_set)\n",
    "            else:\n",
    "                valid_idx.extend(scaffold_set)\n",
    "        else:\n",
    "            train_idx.extend(scaffold_set)\n",
    "\n",
    "    assert len(set(train_idx).intersection(set(valid_idx))) == 0\n",
    "    assert len(set(test_idx).intersection(set(valid_idx))) == 0\n",
    "\n",
    "    train_dataset = dataset[torch.tensor(train_idx)]\n",
    "    valid_dataset = dataset[torch.tensor(valid_idx)]\n",
    "    test_dataset = dataset[torch.tensor(test_idx)]\n",
    "\n",
    "    if not return_smiles:\n",
    "        return train_dataset, valid_dataset, test_dataset\n",
    "    else:\n",
    "        train_smiles = [smiles_list[i][1] for i in train_idx]\n",
    "        valid_smiles = [smiles_list[i][1] for i in valid_idx]\n",
    "        test_smiles = [smiles_list[i][1] for i in test_idx]\n",
    "        return train_dataset, valid_dataset, test_dataset, (train_smiles,\n",
    "                                                            valid_smiles,\n",
    "                                                            test_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81088612-eea1-4172-bef1-c2f56aa06fa2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:09] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [23:36:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:36:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold\n"
     ]
    }
   ],
   "source": [
    "#split\n",
    "smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "print(\"scaffold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32e6d2c1-efb9-4ce4-bd7b-f654357d9118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[24, 2], edge_index=[2, 24], id=[1], x=[13, 2], y=[27])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdc736-b8bb-4ec5-92d0-e4af5445d451",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9990681d-ed90-422e-847a-16cc71e2e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from torch_geometric.data import Data, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4d8e458-8613-440c-8cba-dd166b9e9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collater(object):\n",
    "    def __init__(self, follow_batch, exclude_keys):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def collate(self, batch):\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, Data):\n",
    "            return Batch.from_data_list(batch, self.follow_batch,\n",
    "                                        self.exclude_keys)\n",
    "        elif isinstance(elem, torch.Tensor):\n",
    "            return default_collate(batch)\n",
    "        elif isinstance(elem, float):\n",
    "            return torch.tensor(batch, dtype=torch.float)\n",
    "        elif isinstance(elem, int):\n",
    "            return torch.tensor(batch)\n",
    "        elif isinstance(elem, Mapping):\n",
    "            return {key: self.collate([d[key] for d in batch]) for key in elem}\n",
    "        elif isinstance(elem, tuple) and hasattr(elem, '_fields'):\n",
    "            return type(elem)(*(self.collate(s) for s in zip(*batch)))\n",
    "        elif isinstance(elem, Sequence) and not isinstance(elem, str):\n",
    "            return [self.collate(s) for s in zip(*batch)]\n",
    "\n",
    "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43f321aa-4963-4322-b883-9ee4a9888c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_geometric.data import DataLoader\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    r\"\"\"Data loader which merges data objects from a\n",
    "    :class:`torch_geometric.data.dataset` to a mini-batch.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset from which to load the data.\n",
    "        batch_size (int, optional): How many samples per batch to load.\n",
    "            (default: :obj:`1`)\n",
    "        shuffle (bool, optional): If set to :obj:`True`, the data will be\n",
    "            reshuffled at every epoch. (default: :obj:`False`)\n",
    "        follow_batch (list or tuple, optional): Creates assignment batch\n",
    "            vectors for each key in the list. (default: :obj:`[]`)\n",
    "        exclude_keys (list or tuple, optional): Will exclude each key in the\n",
    "            list. (default: :obj:`[]`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch.utils.data.DataLoader`.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, follow_batch=[],\n",
    "                 exclude_keys=[], **kwargs):\n",
    "\n",
    "        if \"collate_fn\" in kwargs:\n",
    "            del kwargs[\"collate_fn\"]\n",
    "\n",
    "        # Save for PyTorch Lightning...\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        super(DataLoader,\n",
    "              self).__init__(dataset, batch_size, shuffle,\n",
    "                             collate_fn=Collater(follow_batch,\n",
    "                                                 exclude_keys), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8af56881-f928-4403-8493-3dc369a74d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers = args.num_workers)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a18055e-e22e-4d83-8ae5-91f2fb1e2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|▊                                                                        | 1/96 [00:00<00:11,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[202], edge_attr=[422, 2], edge_index=[2, 422], id=[12], ptr=[13], x=[202, 2], y=[324])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(train_loader, desc=\"Iteration\")):\n",
    "    if step==1 : break\n",
    "    else : print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d5ec3bcc-f44f-447a-8e4c-12e97cdb82c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2] # 얘는 Train_dataset에서 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a3dde573-5e92-4019-a295-2b9e1cbb2b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1360,  0.6997, -2.6893,  0.5936,  1.3722,  0.9739,  2.0579, -1.3538,\n",
       "          0.9538,  0.2211, -1.0589,  2.0721, -1.3082, -1.4309,  1.3957,  0.4801,\n",
       "          2.3593, -1.4051,  0.6639,  1.1736,  1.1822,  0.5815, -1.8966,  0.1475,\n",
       "          1.1561,  2.2287,  0.7384],\n",
       "        [ 0.3356,  0.8435, -2.6948,  0.7362,  1.5192,  1.0961,  2.2009, -1.1727,\n",
       "          1.0461,  0.4092, -0.9925,  2.1748, -1.1748, -1.3428,  1.4990,  0.5909,\n",
       "          2.5626, -1.4138,  0.7727,  1.2638,  1.3637,  0.7326, -1.8489,  0.2545,\n",
       "          1.2724,  2.4541,  0.8209],\n",
       "        [-0.1841,  1.0998, -2.5338,  0.3811,  1.1476,  0.5031,  1.4879, -1.6094,\n",
       "          1.1318, -0.2442, -0.9821,  1.9916, -0.7661, -1.6970,  1.0296,  0.0738,\n",
       "          2.2346, -1.4210,  1.6177,  1.1372,  0.5199,  0.7007, -1.5090, -0.4171,\n",
       "          0.6175,  1.8873,  0.5372],\n",
       "        [-0.0212,  0.8391, -2.4188,  0.4983,  1.2013,  0.7632,  1.6879, -1.3786,\n",
       "          0.9997,  0.0719, -0.8919,  1.9005, -0.8378, -1.4584,  1.1671,  0.2381,\n",
       "          2.1480, -1.3316,  1.0521,  1.0563,  0.8714,  0.6058, -1.5711, -0.1101,\n",
       "          0.7932,  1.9323,  0.6276],\n",
       "        [ 0.4192,  0.8982, -2.9082,  0.7904,  1.5890,  1.0756,  2.3938, -1.2088,\n",
       "          1.0361,  0.4879, -1.0368,  2.3224, -1.1646, -1.4068,  1.5643,  0.6749,\n",
       "          2.7527, -1.5694,  0.7481,  1.2661,  1.4599,  0.7789, -2.0358,  0.2856,\n",
       "          1.3305,  2.6423,  0.8153],\n",
       "        [-0.1891,  0.8601, -2.7219,  0.3129,  1.1264,  0.5302,  1.6797, -1.7246,\n",
       "          0.9526, -0.2645, -1.0514,  1.9732, -1.0338, -1.7407,  1.0657,  0.1906,\n",
       "          2.0957, -1.5445,  1.1062,  1.0051,  0.6444,  0.5269, -1.8822, -0.3097,\n",
       "          0.6550,  1.8747,  0.5089],\n",
       "        [ 0.2938,  0.8109, -2.8231,  0.6918,  1.4929,  1.0262,  2.2458, -1.2903,\n",
       "          0.9958,  0.3491, -1.0537,  2.2100, -1.2435, -1.4331,  1.4815,  0.5953,\n",
       "          2.5673, -1.5035,  0.7051,  1.2178,  1.3280,  0.6884, -1.9987,  0.2171,\n",
       "          1.2438,  2.4548,  0.7806],\n",
       "        [ 0.2546,  0.7952, -2.8852,  0.6309,  1.4738,  0.9860,  2.2407, -1.3846,\n",
       "          0.9652,  0.2438, -1.0899,  2.1978, -1.3017, -1.5052,  1.4365,  0.5926,\n",
       "          2.5063, -1.5380,  0.6718,  1.1781,  1.2695,  0.6450, -2.1138,  0.1669,\n",
       "          1.1799,  2.4057,  0.7589],\n",
       "        [ 0.6988,  1.1083, -2.8050,  0.9803,  1.8048,  1.2921,  2.5205, -0.9192,\n",
       "          1.1979,  0.7366, -0.9078,  2.4205, -0.9938, -1.2246,  1.7075,  0.8266,\n",
       "          2.9743, -1.4868,  0.9418,  1.4421,  1.6826,  1.0041, -1.8558,  0.4510,\n",
       "          1.4964,  2.8937,  0.9677],\n",
       "        [ 0.4579,  0.9304, -2.8059,  0.8178,  1.6188,  1.1361,  2.3570, -1.1279,\n",
       "          1.0797,  0.5217, -0.9865,  2.2968, -1.1293, -1.3364,  1.5783,  0.6867,\n",
       "          2.7368, -1.4910,  0.8027,  1.3112,  1.4792,  0.8179, -1.9227,  0.3189,\n",
       "          1.3518,  2.6329,  0.8581],\n",
       "        [ 0.6269,  1.0805, -2.8335,  0.8693,  1.7922,  1.2678,  2.4591, -1.0694,\n",
       "          1.1875,  0.5291, -0.9257,  2.3681, -1.1041, -1.3353,  1.6171,  0.7928,\n",
       "          2.8182, -1.5015,  0.9280,  1.4019,  1.5781,  0.9369, -1.9948,  0.3711,\n",
       "          1.3719,  2.7647,  0.9622],\n",
       "        [ 0.5743,  1.0429, -2.7674,  0.8648,  1.7300,  1.2340,  2.4133, -1.0390,\n",
       "          1.1480,  0.5779, -0.9019,  2.3228, -1.0432, -1.3017,  1.6106,  0.7780,\n",
       "          2.7752, -1.4915,  0.8929,  1.3517,  1.5580,  0.9196, -1.8933,  0.3815,\n",
       "          1.3663,  2.7162,  0.9249]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390892b-594d-4279-974d-6ccabf2892d8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dc331905-d7f7-464f-839a-1b777e07a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, softmax\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn.inits import glorot, zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e4343738-0129-452d-ba3f-c8166727a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Extension of GIN aggregation to incorporate edge information by concatenation.\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int): dimensionality of embeddings for nodes and edges.\n",
    "        embed_input (bool): whether to embed input or not. \n",
    "        \n",
    "\n",
    "    See https://arxiv.org/abs/1810.00826\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, aggr = \"add\"):\n",
    "        super(GINConv, self).__init__()\n",
    "        #multi-layer perceptron\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
    "        self.edge_embedding1 = torch.nn.Embedding(num_bond_type, emb_dim)\n",
    "        self.edge_embedding2 = torch.nn.Embedding(num_bond_direction, emb_dim)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.edge_embedding1.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(self.edge_embedding2.weight.data)\n",
    "        self.aggr = aggr\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        #add self loops in the edge space\n",
    "        edge_index = add_self_loops(edge_index, num_nodes = x.size(0))\n",
    "\n",
    "        #add features corresponding to self-loop edges.\n",
    "        self_loop_attr = torch.zeros(x.size(0), 2)\n",
    "        self_loop_attr[:,0] = 4 #bond type for self-loop edge\n",
    "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
    "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim = 0)\n",
    "\n",
    "        edge_embeddings = self.edge_embedding1(edge_attr[:,0]) + self.edge_embedding2(edge_attr[:,1])\n",
    "\n",
    "        return self.propagate(edge_index[0], x=x, edge_attr=edge_embeddings)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.mlp(aggr_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3520cd57-5c78-4c1e-9a4b-86f5db29274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        num_layer (int): the number of GNN layers\n",
    "        emb_dim (int): dimensionality of embeddings\n",
    "        JK (str): last, concat, max or sum.\n",
    "        max_pool_layer (int): the layer from which we use max pool rather than add pool for neighbor aggregation\n",
    "        drop_ratio (float): dropout rate\n",
    "        gnn_type: gin, gcn, graphsage, gat\n",
    "\n",
    "    Output:\n",
    "        node representations\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, JK = \"last\", drop_ratio = 0, gnn_type = \"gin\"):\n",
    "        super(GNN, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.x_embedding1 = torch.nn.Embedding(num_atom_type, emb_dim)\n",
    "        self.x_embedding2 = torch.nn.Embedding(num_chirality_tag, emb_dim)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.x_embedding1.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(self.x_embedding2.weight.data)\n",
    "\n",
    "        ###List of MLPs\n",
    "        self.gnns = torch.nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            if gnn_type == \"gin\":\n",
    "                self.gnns.append(GINConv(emb_dim, aggr = \"add\"))\n",
    "            elif gnn_type == \"gcn\":\n",
    "                self.gnns.append(GCNConv(emb_dim))\n",
    "            elif gnn_type == \"gat\":\n",
    "                self.gnns.append(GATConv(emb_dim))\n",
    "            elif gnn_type == \"graphsage\":\n",
    "                self.gnns.append(GraphSAGEConv(emb_dim))\n",
    "\n",
    "        ###List of batchnorms\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "    #def forward(self, x, edge_index, edge_attr):\n",
    "    def forward(self, *argv):\n",
    "        if len(argv) == 3:\n",
    "            x, edge_index, edge_attr = argv[0], argv[1], argv[2]\n",
    "        elif len(argv) == 1:\n",
    "            data = argv[0]\n",
    "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        else:\n",
    "            raise ValueError(\"unmatched number of arguments.\")\n",
    "\n",
    "        x = self.x_embedding1(x[:,0]) + self.x_embedding2(x[:,1])\n",
    "\n",
    "        h_list = [x]\n",
    "        for layer in range(self.num_layer):\n",
    "            h = self.gnns[layer](h_list[layer], edge_index, edge_attr)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            #h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "            if layer == self.num_layer - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "            h_list.append(h)\n",
    "\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"concat\":\n",
    "            node_representation = torch.cat(h_list, dim = 1)\n",
    "        elif self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"max\":\n",
    "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
    "            node_representation = torch.max(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
    "        elif self.JK == \"sum\":\n",
    "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
    "            node_representation = torch.sum(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
    "\n",
    "        return node_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8b862220-8048-4c4c-bbe3-830d1810b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_graphpred(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Extension of GIN to incorporate edge information by concatenation.\n",
    "\n",
    "    Args:\n",
    "        num_layer (int): the number of GNN layers\n",
    "        emb_dim (int): dimensionality of embeddings\n",
    "        num_tasks (int): number of tasks in multi-task learning scenario\n",
    "        drop_ratio (float): dropout rate\n",
    "        JK (str): last, concat, max or sum.\n",
    "        graph_pooling (str): sum, mean, max, attention, set2set\n",
    "        gnn_type: gin, gcn, graphsage, gat\n",
    "        \n",
    "    See https://arxiv.org/abs/1810.00826\n",
    "    JK-net: https://arxiv.org/abs/1806.03536\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, num_tasks, JK = \"last\", drop_ratio = 0, graph_pooling = \"mean\", gnn_type = \"gin\"):\n",
    "        super(GNN_graphpred, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.gnn = GNN(num_layer, emb_dim, JK, drop_ratio, gnn_type = gnn_type)\n",
    "\n",
    "        #Different kind of graph pooling\n",
    "        if graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif graph_pooling == \"attention\":\n",
    "            if self.JK == \"concat\":\n",
    "                self.pool = GlobalAttention(gate_nn = torch.nn.Linear((self.num_layer + 1) * emb_dim, 1))\n",
    "            else:\n",
    "                self.pool = GlobalAttention(gate_nn = torch.nn.Linear(emb_dim, 1))\n",
    "        elif graph_pooling[:-1] == \"set2set\":\n",
    "            set2set_iter = int(graph_pooling[-1])\n",
    "            if self.JK == \"concat\":\n",
    "                self.pool = Set2Set((self.num_layer + 1) * emb_dim, set2set_iter)\n",
    "            else:\n",
    "                self.pool = Set2Set(emb_dim, set2set_iter)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        #For graph-level binary classification\n",
    "        if graph_pooling[:-1] == \"set2set\":\n",
    "            self.mult = 2\n",
    "        else:\n",
    "            self.mult = 1\n",
    "        \n",
    "        if self.JK == \"concat\":\n",
    "            self.graph_pred_linear = torch.nn.Linear(self.mult * (self.num_layer + 1) * self.emb_dim, self.num_tasks)\n",
    "        else:\n",
    "            self.graph_pred_linear = torch.nn.Linear(self.mult * self.emb_dim, self.num_tasks)\n",
    "\n",
    "    def from_pretrained(self, model_file):\n",
    "        #self.gnn = GNN(self.num_layer, self.emb_dim, JK = self.JK, drop_ratio = self.drop_ratio)\n",
    "        self.gnn.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def forward(self, *argv):\n",
    "        if len(argv) == 4:\n",
    "            x, edge_index, edge_attr, batch = argv[0], argv[1], argv[2], argv[3]\n",
    "        elif len(argv) == 1:\n",
    "            data = argv[0]\n",
    "            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        else:\n",
    "            raise ValueError(\"unmatched number of arguments.\")\n",
    "\n",
    "        node_representation = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        return self.graph_pred_linear(self.pool(node_representation, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3b9c7df-dd65-43a8-82de-64dc0df91829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_graphpred(\n",
       "  (gnn): GNN(\n",
       "    (x_embedding1): Embedding(120, 300)\n",
       "    (x_embedding2): Embedding(3, 300)\n",
       "    (gnns): ModuleList(\n",
       "      (0): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (1): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (2): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (3): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (4): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a2f9ab2-a52e-4022-95dc-562164ae434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_param_group = []\n",
    "model_param_group.append({\"params\": model.gnn.parameters()})\n",
    "if args.graph_pooling == \"attention\":\n",
    "    model_param_group.append({\"params\": model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "model_param_group.append({\"params\": model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "optimizer = optim.Adam(model_param_group, lr=args.lr, weight_decay=args.decay)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade0d11-f730-4d2e-a2d9-f85f2ff3dc39",
   "metadata": {},
   "source": [
    "# cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed47264-45d1-484d-bd17-e9321384412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3464be7c-dd24-484c-9600-035e1cdb75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    iter_count = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        y = batch.y.view(pred.shape).to(torch.float64)\n",
    "\n",
    "        #loss matrix after removing null target\n",
    "        loss = calcul_loss(pred, y, criterion)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss\n",
    "        iter_count += 1\n",
    "        \n",
    "    return loss_sum / iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f214cfbc-f2b8-433e-9bba-da07afeeb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    cum_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            y = batch.y.view(pred.shape)\n",
    "            loss = calcul_loss(pred, y, criterion)\n",
    "        \n",
    "        cum_loss += loss\n",
    "        y_true.append(y)\n",
    "        y_scores.append(pred)\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    y_scores = torch.cat(y_scores, dim = 0).cpu().numpy()\n",
    "\n",
    "    roc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        #AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == -1) > 0:\n",
    "            is_valid = y_true[:,i]**2 > 0\n",
    "            roc_list.append(roc_auc_score((y_true[is_valid,i] + 1)/2, y_scores[is_valid,i]))\n",
    "\n",
    "    if len(roc_list) < y_true.shape[1]:\n",
    "        print(\"Some target is missing!\")\n",
    "        print(\"Missing ratio: %f\" %(1 - float(len(roc_list))/y_true.shape[1]))\n",
    "\n",
    "    return cum_loss, sum(roc_list)/len(roc_list) #y_true.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f689703b-1f45-44d5-a8d1-a92ddb6fd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    cum_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(test_loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            y = batch.y.view(pred.shape)\n",
    "            loss = calcul_loss(pred, y, criterion)\n",
    "\n",
    "        y_true.append(y)\n",
    "        y_scores.append(pred)\n",
    "        cum_loss += loss\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).cpu().numpy()\n",
    "    y_scores = torch.cat(y_scores, dim = 0).cpu().numpy()\n",
    "\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    rec_list = []\n",
    "    prec_list = []\n",
    "    f1s_list = []\n",
    "    BA_list = []\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    tn_list = []\n",
    "    fn_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = confusion_mat(y_true[:,i], y_scores[:,i])\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "        rec_list.append(rec)\n",
    "        prec_list.append(prec)\n",
    "        f1s_list.append(f1s)\n",
    "        BA_list.append(BA)\n",
    "        tp_list.append(tp)\n",
    "        fp_list.append(fp)\n",
    "        tn_list.append(tn)\n",
    "        fn_list.append(fn)\n",
    "\n",
    "    return cum_loss, auc_list, acc_list, rec_list, prec_list, f1s_list, BA_list, tp_list, fp_list, tn_list, fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b7c49a7-ee6e-4891-b714-2680a5af9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "    #set up dataset\n",
    "    dataset = MoleculeDataset_other(\"dataset/\" + args.dataset, dataset=args.dataset)\n",
    "    num_tasks = len(dataset[0]['y'])\n",
    "\n",
    "    \n",
    "    if args.split == \"scaffold\":\n",
    "        smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "        print(f'scaffold_balanced_split')\n",
    "    elif args.split == \"random\":\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        print(\"random\")\n",
    "    elif args.split == \"random_scaffold\":\n",
    "        smiles_list = pd.read_csv('dataset/' + args.dataset + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "        train_dataset, valid_dataset, test_dataset = random_scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1, seed = args.seed)\n",
    "        print(\"random scaffold\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split option.\")\n",
    "        \n",
    "    print(f'total_size:{len(dataset)} train_size:{len(train_dataset)} val_size:{len(valid_dataset)} test_size:{len(test_dataset)}')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n",
    "    val_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "\n",
    "    #set up model\n",
    "    model = GNN_graphpred(args.num_layer, args.emb_dim, num_tasks, JK = args.JK, drop_ratio = args.dropout_ratio, graph_pooling = args.graph_pooling, gnn_type = args.gnn_type)\n",
    "    if not args.input_model_file == \"\":\n",
    "        model.from_pretrained(args.input_model_file)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    #set up optimizer\n",
    "    #different learning rate for different part of GNN\n",
    "    model_param_group = []\n",
    "    model_param_group.append({\"params\": model.gnn.parameters()})\n",
    "    if args.graph_pooling == \"attention\":\n",
    "        model_param_group.append({\"params\": model.pool.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "    model_param_group.append({\"params\": model.graph_pred_linear.parameters(), \"lr\":args.lr*args.lr_scale})\n",
    "    optimizer = optim.Adam(model_param_group, lr=args.lr, weight_decay=args.decay)\n",
    "    print(optimizer)\n",
    "\n",
    "    best_val_loss = 9999\n",
    "    best_model_path = os.path.join(args.output_path, str(args.seed))\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        print(\"====epoch \" + str(epoch))\n",
    "        \n",
    "        train_loss = train(args, model, device, train_loader, optimizer)\n",
    "\n",
    "        print(\"====Evaluation\")\n",
    "        val_loss, val_auc = valid(args, model, device, val_loader)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_cp(args, model, path=best_model_path)\n",
    "        print(f'train_loss:{train_loss:.4f} val_loss:{val_loss:.4f} val_auc:{val_auc:.4f}')\n",
    "    \n",
    "    best_state = torch.load(os.path.join(best_model_path,'model.pt'))\n",
    "    model.load_state_dict(best_state['state_dict'])\n",
    "    \n",
    "    test_loss, auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = test(args, model, device, test_loader)\n",
    "    avg_auc = sum(auc)/num_tasks\n",
    "    avg_acc = sum(acc)/num_tasks\n",
    "    avg_rec = sum(rec)/num_tasks\n",
    "    avg_prec = sum(prec)/num_tasks\n",
    "    avg_f1s = sum(f1s)/num_tasks\n",
    "    avg_BA = sum(BA)/num_tasks\n",
    "    avg_tp = sum(tp)/num_tasks\n",
    "    avg_fp = sum(fp)/num_tasks\n",
    "    avg_tn = sum(tn)/num_tasks\n",
    "    avg_fn = sum(fn)/num_tasks\n",
    "        \n",
    "    print(f'seed:{args.seed} loss:{test_loss} auc:{avg_auc} acc:{avg_acc} rec:{avg_rec} prec:{avg_prec} f1:{avg_f1s} BA:{avg_BA}\\ntp:{avg_tp} fp:{avg_fp} fn:{avg_fn} tn:{avg_tn}')\n",
    "    #delete for memory\n",
    "    del train_dataset, valid_dataset, test_dataset, train_loader, val_loader, test_loader\n",
    "\n",
    "    return avg_auc, avg_acc, avg_rec, avg_prec, avg_f1s, avg_BA, avg_tp, avg_fp, avg_tn, avg_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9038c646-4c22-44df-9df7-e544507bd1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b948528b-3986-4aef-bc86-95cf4cdedad1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "[08:06:14] WARNING: not removing hydrogen atom without neighbors\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold_balanced_split\n",
      "total_size:7831 train_size:6264 val_size:783 test_size:784\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "====epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 196/196 [00:03<00:00, 52.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 40.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.5212 val_loss:13.0586 val_auc:0.5935\n",
      "====epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 196/196 [00:03<00:00, 52.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 41.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.3220 val_loss:9.6046 val_auc:0.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 43.65it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "[08:06:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:0 loss:9.561942131759293 auc:0.5999736823705634 acc:0.9003052888258489 rec:0.007285814606741573 prec:0.1527777777777778 f1:0.013822434875066453 BA:0.5035645865013657\n",
      "tp:0.6666666666666666 fp:0.08333333333333333 fn:55.583333333333336 tn:532.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold_balanced_split\n",
      "total_size:7831 train_size:6264 val_size:783 test_size:784\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "====epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 196/196 [00:03<00:00, 52.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 42.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.5412 val_loss:13.3269 val_auc:0.5894\n",
      "====epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 196/196 [00:03<00:00, 50.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 42.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.3273 val_loss:10.0504 val_auc:0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 40.82it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "[08:06:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:1 loss:10.036050189019925 auc:0.6147799217739862 acc:0.8995790877408466 rec:0.002740714731585518 prec:0.13888888888888887 f1:0.005341401464216345 BA:0.5012920365637877\n",
      "tp:0.25 fp:0.08333333333333333 fn:56.0 tn:532.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaffold_balanced_split\n",
      "total_size:7831 train_size:6264 val_size:783 test_size:784\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-07\n",
      ")\n",
      "====epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 196/196 [00:03<00:00, 51.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.5129 val_loss:12.2664 val_auc:0.6082\n",
      "====epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/196 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 196/196 [00:03<00:00, 53.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.3173 val_loss:9.3715 val_auc:0.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/25 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "Iteration: 100%|██████████| 25/25 [00:00<00:00, 37.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:2 loss:9.36351836373875 auc:0.5988983900266698 acc:0.8997281198325644 rec:0.006281210986267165 prec:0.11805555555555554 f1:0.011912291798439806 BA:0.5027834240458667\n",
      "tp:0.5833333333333334 fp:0.3333333333333333 fn:55.666666666666664 tn:532.5\n",
      "test end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.output_path):\n",
    "    os.makedirs(args.output_path)\n",
    "auc_list = []\n",
    "acc_list = []\n",
    "rec_list = []\n",
    "prec_list = []\n",
    "f1s_list = []\n",
    "BA_list = []\n",
    "tp_list = []\n",
    "fp_list = []\n",
    "tn_list = []\n",
    "fn_list = []\n",
    "for k in range(3):\n",
    "    auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn = cross_validate(args)\n",
    "    auc_list.append(auc)\n",
    "    acc_list.append(acc)\n",
    "    rec_list.append(rec)\n",
    "    prec_list.append(prec)\n",
    "    f1s_list.append(f1s)\n",
    "    BA_list.append(BA)\n",
    "    tp_list.append(tp)\n",
    "    fp_list.append(fp)\n",
    "    tn_list.append(tn)\n",
    "    fn_list.append(fn)\n",
    "    args.seed += 1\n",
    "print(f'test end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ac081e0-5e42-44a5-8d86-9054bd0d8755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall test_auc : 0.6046\n",
      "std=0.0072\n",
      "overall test_accuracy : 0.8999\n",
      "std=0.0003\n",
      "overall test_recall : 0.0054\n",
      "std=0.0019\n",
      "overall test_precision : 0.1366\n",
      "std=0.0143\n",
      "overall test_f1score : 0.0104\n",
      "std=0.0036\n",
      "overall test_Balanced_Accuracy : 0.5025\n",
      "std=0.0009\n",
      "overall test_tp : 0.50\n",
      "std=0.18\n",
      "overall test_fp : 0.17\n",
      "std=0.12\n",
      "overall test_fn : 55.75\n",
      "std=0.18\n",
      "overall test_tn : 532.67\n",
      "std=0.12\n",
      "\n",
      "       (pred)pos    neg(pred)\n",
      "pos(true)    0.58  55.67\n",
      "neg(true)    0.33  532.50\n"
     ]
    }
   ],
   "source": [
    "print(f'overall test_auc : {np.nanmean(auc_list):.4f}\\nstd={np.nanstd(auc_list):.4f}')\n",
    "print(f'overall test_accuracy : {np.nanmean(acc_list):.4f}\\nstd={np.nanstd(acc_list):.4f}')\n",
    "print(f'overall test_recall : {np.nanmean(rec_list):.4f}\\nstd={np.nanstd(rec_list):.4f}')\n",
    "print(f'overall test_precision : {np.nanmean(prec_list):.4f}\\nstd={np.nanstd(prec_list):.4f}')\n",
    "print(f'overall test_f1score : {np.nanmean(f1s_list):.4f}\\nstd={np.nanstd(f1s_list):.4f}')\n",
    "print(f'overall test_Balanced_Accuracy : {np.nanmean(BA_list):.4f}\\nstd={np.nanstd(BA_list):.4f}')\n",
    "print(f'overall test_tp : {np.nanmean(tp_list):.2f}\\nstd={np.nanstd(tp_list):.2f}')\n",
    "print(f'overall test_fp : {np.nanmean(fp_list):.2f}\\nstd={np.nanstd(fp_list):.2f}')\n",
    "print(f'overall test_fn : {np.nanmean(fn_list):.2f}\\nstd={np.nanstd(fn_list):.2f}')\n",
    "print(f'overall test_tn : {np.nanmean(tn_list):.2f}\\nstd={np.nanstd(tn_list):.2f}')\n",
    "print(f'\\n       (pred)pos    neg(pred)')\n",
    "print(f'pos(true)    {tp:.2f}  {fn:.2f}')\n",
    "print(f'neg(true)    {fp:.2f}  {tn:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cf4eaf2-1a81-4678-ad9c-4d05f1098685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc : 0.6045506647237398\n",
      "std=0.007246485964761184\n"
     ]
    }
   ],
   "source": [
    "print(f'test_auc : {np.nanmean(auc_list)}\\nstd={np.nanstd(auc_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd9a61c1-f38d-4b00-9c80-eb16d4126275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08178562764256868"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanstd([0.71, 0.60, 0.80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b98e3785-d5d1-469b-a7e6-2316e5239f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cp(args, model, path=None):\n",
    "    model_state = {\n",
    "        'args': args,\n",
    "        'state_dict': model.state_dict(),\n",
    "    }\n",
    "    if path is not None : \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(model_state, os.path.join(path, 'model.pt'))\n",
    "    else : \n",
    "        if not os.path.exists(args.output_path):\n",
    "            os.makedirs(args.output_path)\n",
    "        torch.save(model_state, os.path.join(args.output_path,'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfe0d71d-7c9a-41a9-a158-c8d3d0dc9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, mean_absolute_error, r2_score, \\\n",
    "    precision_recall_curve, auc, recall_score, confusion_matrix, f1_score, precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "810d622a-7bdd-4a1f-b028-26268e84c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(targets: List[int], preds: List[float], threshold: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Computes the specificity of a binary prediction task using a given threshold for generating hard predictions.\n",
    "\n",
    "    :param targets: A list of binary targets.\n",
    "    :param preds: A list of prediction probabilities.\n",
    "    :param threshold: The threshold above which a prediction is a 1 and below which (inclusive) a prediction is a 0\n",
    "    :return: The computed specificity.\n",
    "    \"\"\"\n",
    "    valid = targets**2>0\n",
    "    targets = (targets[valid]+1)/2\n",
    "    preds = preds[valid]\n",
    "    hard_preds = [1 if p > threshold else 0 for p in preds]\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(targets, hard_preds).ravel()\n",
    "    acc = accuracy_score(targets, hard_preds)\n",
    "    rec = recall_score(targets, hard_preds)\n",
    "    prec = precision_score(targets, hard_preds)\n",
    "    spe = tn / float(tn + fp)\n",
    "    f1s = f1_score(targets, hard_preds)\n",
    "    BA = (rec+spe)/2\n",
    "    return auc, acc, rec, prec, f1s, BA, tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97770c-3073-4692-87ce-260effdd5235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
